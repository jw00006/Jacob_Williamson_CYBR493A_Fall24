{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c829b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Web scrapping in Python\n",
    "This is an example of scrapping a web page in Python using beautiful soup\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "class WVUScraper:\n",
    "    \"\"\"\n",
    "    A web scraper class for scraping the WVU website.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    base_url : str\n",
    "        The base URL of the site to scrape (e.g., \"https://www.wvu.edu\").\n",
    "\n",
    "    Methods:\n",
    "    -------\n",
    "    get_html(url):\n",
    "        Sends a GET request to the URL and retrieves the HTML content.\n",
    "\n",
    "    parse_html(html):\n",
    "        Parses HTML content using BeautifulSoup.\n",
    "\n",
    "    extract_links(soup):\n",
    "        Extracts all anchor tags and retrieves link text and URLs.\n",
    "\n",
    "    extract_headings(soup):\n",
    "        Extracts all headings (h1, h2, h3) from the parsed HTML content.\n",
    "\n",
    "    scrape_page(url):\n",
    "        Scrapes a single page for links and headings.\n",
    "\n",
    "    follow_links(url, max_pages=5, delay=2):\n",
    "        Follows links on the page up to a specified number of pages.\n",
    "\n",
    "    grab_specific_item(soup, selector):\n",
    "        Extracts specific content based on a CSS selector (e.g., class, id).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def get_html(self, url):\n",
    "        \"\"\"\n",
    "        Fetches HTML content from a given URL.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        url : str\n",
    "            The URL to retrieve HTML from.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        str\n",
    "            HTML content if the request is successful; None otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            return response.content\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def parse_html(self, html):\n",
    "        \"\"\"\n",
    "        Parses HTML content using BeautifulSoup.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        html : str\n",
    "            Raw HTML content to be parsed.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        BeautifulSoup\n",
    "            A BeautifulSoup object of the parsed HTML.\n",
    "        \"\"\"\n",
    "        return BeautifulSoup(html, 'html.parser') if html else None\n",
    "\n",
    "    def extract_links(self, soup):\n",
    "        \"\"\"\n",
    "        Extracts all anchor tags and retrieves link text and URLs.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        soup : BeautifulSoup\n",
    "            Parsed HTML content.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        list of dict\n",
    "            A list of dictionaries containing link text and URLs.\n",
    "        \"\"\"\n",
    "        links = []\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            links.append({\n",
    "                'text': link.text.strip(),\n",
    "                'url': link.get('href')\n",
    "            })\n",
    "        return links\n",
    "\n",
    "    def extract_headings(self, soup):\n",
    "        \"\"\"\n",
    "        Extracts all headings (h1, h2, h3) from the parsed HTML content.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        soup : BeautifulSoup\n",
    "            Parsed HTML content.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary with heading levels as keys and a list of text for each heading as values.\n",
    "        \"\"\"\n",
    "        headings = {}\n",
    "        for level in ['h1', 'h2', 'h3']:\n",
    "            headings[level] = [heading.text.strip() for heading in soup.find_all(level)]\n",
    "        return headings\n",
    "\n",
    "    def scrape_page(self, url):\n",
    "        \"\"\"\n",
    "        Scrapes a single page for links and headings.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        url : str\n",
    "            The URL of the page to scrape.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary containing the page URL, extracted links, and headings.\n",
    "        \"\"\"\n",
    "        html = self.get_html(url)\n",
    "        if not html:\n",
    "            return None\n",
    "\n",
    "        soup = self.parse_html(html)\n",
    "        data = {\n",
    "            'url': url,\n",
    "            'links': self.extract_links(soup),\n",
    "            'headings': self.extract_headings(soup),\n",
    "        }\n",
    "        return data\n",
    "\n",
    "    def follow_links(self, url, max_pages=5, delay=2):\n",
    "        \"\"\"\n",
    "        Follows links on the main page and scrapes each one up to a specified number of pages.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        url : str\n",
    "            The initial URL to start scraping from.\n",
    "\n",
    "        max_pages : int, optional\n",
    "            The maximum number of pages to scrape (default is 5).\n",
    "\n",
    "        delay : int, optional\n",
    "            Delay in seconds between requests to avoid server overload (default is 2).\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        list of dict\n",
    "            A list of dictionaries, each containing scraped data from a page.\n",
    "        \"\"\"\n",
    "        main_data = self.scrape_page(url)\n",
    "        if not main_data:\n",
    "            return []\n",
    "\n",
    "        all_data = [main_data]\n",
    "        visited_urls = {url}\n",
    "\n",
    "        for link in main_data['links']:\n",
    "            full_url = link['url'] if link['url'].startswith('http') else f\"{self.base_url}{link['url']}\"\n",
    "            if full_url not in visited_urls and len(all_data) < max_pages:\n",
    "                print(f\"Scraping {full_url}\")\n",
    "                page_data = self.scrape_page(full_url)\n",
    "                if page_data:\n",
    "                    all_data.append(page_data)\n",
    "                    visited_urls.add(full_url)\n",
    "                time.sleep(delay)\n",
    "        return all_data\n",
    "\n",
    "    def grab_specific_item(self, soup, selector):\n",
    "        \"\"\"\n",
    "        Extracts specific content based on a CSS selector.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        soup : BeautifulSoup\n",
    "            Parsed HTML content.\n",
    "\n",
    "        selector : str\n",
    "            The CSS selector for the item to grab (e.g., '.class' or '#id').\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        list\n",
    "            A list of strings of content matching the selector.\n",
    "        \"\"\"\n",
    "        items = soup.select(selector)\n",
    "        return [item.get_text(strip=True) for item in items]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3c24d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://bugzilla.redhat.com/saml2_login.cgi?idp=Fedora%20Account%20System&target=index.cgi\n",
      "Error fetching https://bugzilla.redhat.com/saml2_login.cgi?idp=Fedora%20Account%20System&target=index.cgi: 401 Client Error: Unauthorized for url: https://id.fedoraproject.org/login/gssapi/negotiate?ipsilon_transaction_id=5ac1af3b-b5f4-43df-9d4e-0a44e5a88def\n",
      "Scraping https://bugzilla.redhat.com/saml2_login.cgi?idp=Red%20Hat%20Associate&target=index.cgi\n",
      "Error fetching https://bugzilla.redhat.com/saml2_login.cgi?idp=Red%20Hat%20Associate&target=index.cgi: 401 Client Error: Unauthorized for url: https://auth.redhat.com/auth/realms/EmployeeIDP/protocol/saml?SAMLRequest=hVLLbtswELznKwjeLYmyUDuE5cCNW9RAWgRO0kMvxZpaxwT4ULmUW%2FfrQwkVGqBCyhvJnZ2dmV3d%2FLKGnTGQ9q7mIis4owiuAeMd1vyCxG%2FWVysCa1q56eLJ7fFHhxRZAjqSw0fNu%2BCkB9IkHVgkGZV82Hy%2Bk2VWyDb46JU3%2FIpNnL9t3u4CRBhimnK6zWb8vvWOOovhAcNZK3za39X8FGNLMs8P3fNvbQxkAZsTxEx5m%2FfM5XdQlKlnzdmOqMOd6z2INS%2BLspoJMRPlo6jkXEhRfZvm321rPgc4LubLRbMsGnVUVSWK40IcDvBucV1eL2EaeR%2F8WTcYviTJNd9jwz5BZO%2F%2FjDqN%2BToGlpyZrtimiLSDOFSN%2BiHl91p7f88DgrGUf7Ct8RfE3fY%2BHxMbzOHrRDAsgBzMCev%2F22kxQgMRek9X%2BWvs2KuVvd5E5o1WF%2FbRBwvx7Q3oX3QzOw6lsu0toIgucrYxxv%2B8TUJiclBwlqeNzf9d2fUL&RelayState=https%3A%2F%2Fbugzilla.redhat.com%2Findex.cgi&SigAlg=http%3A%2F%2Fwww.w3.org%2F2000%2F09%2Fxmldsig%23rsa-sha1&Signature=SPvni4cyFLC%2FIX3dK1my4sEIubTjlk8EcVx7ImbAHvirnmiuanCWKX49%2FSmgMEc7dx2mQSDEXQEOAt%2F%2FOyDdkchgzryHslDPYAByBHLLKKwwl5SVOf1NVjUZ9fhJAcjeoLkJOyKwaoIO7yptGOMIwwGh8wIIkAerzhLpz8yp%2Bvwb66GYqf6j8hB62pLZPqemTg5g5tTnBFrrksrSVPZDrsCOyhNW1CDbuJbaGlzEEYYoVw1rM29Uqk53swXjOipEPsz4iewHOyQbAVRX9aP1A1FMF6JuLb%2BRwtaSL%2FKr96E%2Fr61B%2BuHS6BrMWhJwR3C9grE9ewQG62oBzg%2FSVBjDXxlUmz2RX9H9w2%2F4ja864uC4fN%2FAeOoQexqfbCrsEGy7%2BA0%2BlVKeiaVQMfIFOQYDVNVZtzB0n%2FbT1Z7Jv0c9mAJTEDVrNloVgCg0b2we7%2FgMv32uG%2BlZGJ3EPoR%2FY0xm8DcT2%2Bwd1dEvfSFwY1n2fFLxuXVDygx09lFRB4YuVALo3Nl%2FA%2BverNZLN1cI6JwmKWsAOn3TFak6M1pa4BjZBYgrW400pHmRE361%2FRSxHhO7PNA2Ef%2Fg8NWY%2Be3f1AbD9FmNHU6saA%2B4%2FCLf1qp3YbYzEsC4FIk0nUXu5Gg3lLTR49sPgN09kiMzgkmjczb6qFqNoKSKoo6SGJ6UA1Z8M3U%3D\n",
      "Scraping https://bugzilla.redhat.com/saml2_login.cgi?idp=Red%20Hat%20Customer&target=index.cgi\n",
      "Scraping https://bugzilla.redhat.com/createaccount.cgi\n",
      "Main Heading: []\n",
      "About Section Paragraphs:\n"
     ]
    }
   ],
   "source": [
    "import Web_Scraping_beautiful_soup as ws\n",
    "\n",
    "\n",
    "# Main function to initialize and run the scraper\n",
    "def main():\n",
    "    # Initialize the scraper with the base URL\n",
    "    scraper = ws.WVUScraper(\"https://bugzilla.redhat.com/\")\n",
    "\n",
    "    # Scrape the main page and follow links\n",
    "    data = scraper.follow_links(scraper.base_url, max_pages=3, delay=1)\n",
    "\n",
    "    # # Print data for each page\n",
    "    # for page in data:\n",
    "    #     print(\"Page URL:\", page['url'])\n",
    "    #     print(\"Headings:\", page['headings'])\n",
    "    #     print(\"Links:\", page['links'])\n",
    "\n",
    "    # Example of grabbing a specific item using a CSS selector\n",
    "    html = scraper.get_html(scraper.base_url)\n",
    "    soup = scraper.parse_html(html)\n",
    "\n",
    "    # Attempt to retrieve the main heading using an alternative selector\n",
    "    main_heading = scraper.grab_specific_item(soup, \".dataTables_info\")\n",
    "    print(\"Main Heading:\", main_heading)\n",
    "\n",
    "    main_heading = scraper.grab_specific_item(soup, \".wvu-nav-wrapper\")\n",
    "    for something in main_heading:\n",
    "        print( something,'\\t')\n",
    "\n",
    "    # Attempt to retrieve paragraphs within the about section using an alternative selector\n",
    "    about_paragraphs = scraper.grab_specific_item(soup, \".about-content p\")\n",
    "    print(\"About Section Paragraphs:\")\n",
    "    for para in about_paragraphs:\n",
    "        print(para)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b104665",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2085600238.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    FIND IDs / HEADINGS / NUMBER OF ENTRIES\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "FIND IDs / HEADINGS / NUMBER OF ENTRIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8e9925c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3379066740.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    HAVE DONE BEFORE THURSDAY CLASS\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "HAVE DONE BEFORE THURSDAY CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0839aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pythonForCyber)",
   "language": "python",
   "name": "pythonforcyber"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
